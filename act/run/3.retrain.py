import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import h5py
import os
import pickle
import torchvision.transforms as transforms
from torchvision.models import resnet18
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# ================= é…ç½® =================
DATA_DIR = "../../data_act"        # æ•°æ®ç›®å½•
CKPT_DIR = "model_finetune"        # ã€å»ºè®®ä¿®æ”¹ã€‘å»ºè®®æ”¹ä¸ºæ–°çš„æ–‡ä»¶å¤¹åå­—ï¼ˆä¾‹å¦‚ model_v2ï¼‰ï¼Œè¿™æ ·ç»å¯¹å®‰å…¨ï¼Œä¸ä¼šè¦†ç›–æ—§æ¨¡å‹
                                   # å¦‚æœä½ åšæŒç”¨åŸæ¥çš„ "model" æ–‡ä»¶å¤¹ï¼Œè¯·åŠ¡å¿…è®¾ç½®æ­£ç¡®çš„ START_EPOCH

# --- æ ¸å¿ƒä¿®æ”¹ï¼šé¢„è®­ç»ƒ/æ–­ç‚¹ç»­è®­è®¾ç½® ---
RESUME_CHECKPOINT = "model/policy_last.pth"  # ã€æ–°å¢ã€‘è¿™é‡Œå¡«ä½ ä¹‹å‰è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ã€‚å¦‚æœå¡« Noneï¼Œåˆ™ä»å¤´è®­ç»ƒ
START_EPOCH = 20                             # ã€æ–°å¢ã€‘å»ºè®®å¡«ä¹‹å‰çš„è½®æ•°ï¼ˆä¾‹å¦‚ 50ï¼‰ï¼Œè¿™æ ·ä¿å­˜çš„æ–‡ä»¶åä¼šé¡ºå»¶

BATCH_SIZE = 128               # 4090 æ˜¾å¡å»ºè®® 128
NUM_EPOCHS = 80                # æ¥ç€å†è®­ç»ƒå¤šå°‘è½®
LR = 1e-5                      # å­¦ä¹ ç‡ (å¾®è°ƒå»ºè®® 1e-5ï¼Œé˜²æ­¢ç ´åå·²æœ‰èƒ½åŠ›)
CHUNK_SIZE = 60                # é¢„æµ‹æœªæ¥ 60 æ­¥ (ç¼©çŸ­æ­¥é•¿ä»¥å¢åŠ ç¨³å®šæ€§)
KL_WEIGHT = 20                 # CVAE KL æ•£åº¦æƒé‡ (å¢åŠ æƒé‡ä»¥å‡å°‘æŠ–åŠ¨)
NUM_WORKERS = 16               # 4090 æ˜¾å¡å»ºè®® 8 æˆ– 16

# ================= 0. è®¾å¤‡æ£€æµ‹ä¸æ‰“å° =================
def get_device():
    if torch.cuda.is_available():
        d = torch.device("cuda")
        name = torch.cuda.get_device_name(0)
        print(f"\nâœ… æ£€æµ‹åˆ° GPU: {name}")
        print(f"ğŸš€ å°†ä½¿ç”¨ CUDA è¿›è¡ŒåŠ é€Ÿè®­ç»ƒ\n")
    else:
        d = torch.device("cpu")
        print(f"\nâš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œæ­£åœ¨ä½¿ç”¨ CPU")
        print(f"ğŸ¢ è®­ç»ƒé€Ÿåº¦å¯èƒ½ä¼šè¾ƒæ…¢\n")
    return d

DEVICE = get_device()

if not os.path.exists(CKPT_DIR):
    os.makedirs(CKPT_DIR)

# ================= 1. æ•°æ®é¢„å¤„ç†å·¥å…· =================
def get_dataset_stats(data_dir):
    """ç»Ÿè®¡æ•´ä¸ªæ•°æ®é›†çš„ Mean å’Œ Std"""
    # ä¼˜å…ˆæ£€æŸ¥ CKPT_DIR ä¸‹æ˜¯å¦æœ‰ç»Ÿè®¡é‡
    stats_path = os.path.join(CKPT_DIR, "dataset_stats.pkl")
    
    # å°è¯•å» RESUME_CHECKPOINT æ‰€åœ¨çš„æ–‡ä»¶å¤¹æ‰¾ç»Ÿè®¡é‡ï¼ˆé˜²æ­¢é‡å¤è®¡ç®—ï¼‰
    if not os.path.exists(stats_path) and RESUME_CHECKPOINT:
        old_dir = os.path.dirname(RESUME_CHECKPOINT)
        old_stats = os.path.join(old_dir, "dataset_stats.pkl")
        if os.path.exists(old_stats):
            print(f"ğŸ”„ ä»æ—§æ¨¡å‹ç›®å½•å¤åˆ¶ç»Ÿè®¡é‡: {old_stats}")
            with open(old_stats, 'rb') as f:
                stats = pickle.load(f)
            # ä¿å­˜åˆ°æ–°ç›®å½•
            with open(stats_path, 'wb') as f:
                pickle.dump(stats, f)
            return stats

    if os.path.exists(stats_path):
        print(f"ğŸ”„ å‘ç°å·²å­˜åœ¨çš„ç»Ÿè®¡é‡æ–‡ä»¶: {stats_path}ï¼Œç›´æ¥åŠ è½½...")
        with open(stats_path, 'rb') as f:
            return pickle.load(f)

    print("ğŸ“Š æœªå‘ç°ç»Ÿè®¡é‡ï¼Œæ­£åœ¨é‡æ–°è®¡ç®— (Normalization Stats)...")
    all_qpos = []
    all_action = []
    
    files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.hdf5')]
    
    for f_path in tqdm(files):
        with h5py.File(f_path, 'r') as f:
            all_qpos.append(f['/observations/qpos'][()])
            all_action.append(f['/action'][()])
    
    # æ‹¼æ¥æ‰€æœ‰æ•°æ®
    all_qpos = np.concatenate(all_qpos, axis=0)
    all_action = np.concatenate(all_action, axis=0)
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    stats = {
        'qpos_mean': torch.from_numpy(np.mean(all_qpos, axis=0)).float(),
        'qpos_std': torch.from_numpy(np.std(all_qpos, axis=0)).float(),
        'action_mean': torch.from_numpy(np.mean(all_action, axis=0)).float(),
        'action_std': torch.from_numpy(np.std(all_action, axis=0)).float()
    }
    
    # é˜²æ­¢ std ä¸º 0
    stats['qpos_std'] = torch.clip(stats['qpos_std'], 1e-2, None)
    stats['action_std'] = torch.clip(stats['action_std'], 1e-2, None)
    
    # ä¿å­˜
    with open(stats_path, 'wb') as f:
        pickle.dump(stats, f)
    
    print("âœ… ç»Ÿè®¡å®Œæˆå¹¶ä¿å­˜ï¼")
    return stats

# ================= 2. æ•°æ®é›†ç±» =================
class ACTDataset(Dataset):
    def __init__(self, data_dir, stats):
        self.stats = stats
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # --- æ„å»ºç´¢å¼• ---
        self.indices = []
        files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.hdf5')])
        
        print("ğŸ“‚ æ­£åœ¨æ‰«ææ•°æ®é›†ï¼Œæ„å»ºç´¢å¼•...")
        for f_path in tqdm(files):
            with h5py.File(f_path, 'r') as f:
                # è·å–è¯¥ Episode çš„æ€»å¸§æ•°
                total_frames = f['/action'].shape[0]
                # å°†æ¯ä¸€å¸§éƒ½åŠ å…¥ç´¢å¼•
                for i in range(total_frames):
                    self.indices.append((f_path, i))
        
        print(f"ğŸ‰ æ•°æ®é›†æ„å»ºå®Œæˆ! å…±æœ‰ {len(files)} ä¸ªæ–‡ä»¶ï¼Œå±•å¼€ä¸º {len(self.indices)} ä¸ªè®­ç»ƒæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        file_path, start_ts = self.indices[idx]
        
        with h5py.File(file_path, 'r') as f:
            # 1. è¯»å–å½“å‰å¸§è§‚æµ‹
            qpos = torch.from_numpy(f['/observations/qpos'][start_ts]).float()
            img_top = f['/observations/images/top'][start_ts] 
            
            # 2. è¯»å–æœªæ¥åŠ¨ä½œåºåˆ—
            action_len = f['/action'].shape[0]
            end_ts = start_ts + CHUNK_SIZE
            
            if end_ts <= action_len:
                action = torch.from_numpy(f['/action'][start_ts:end_ts]).float()
                is_pad = torch.zeros(CHUNK_SIZE)
            else:
                real_len = action_len - start_ts
                action_real = torch.from_numpy(f['/action'][start_ts:]).float()
                last_action = action_real[-1].unsqueeze(0)
                pad_len = CHUNK_SIZE - real_len
                action_pad = last_action.repeat(pad_len, 1)
                
                action = torch.cat([action_real, action_pad], dim=0)
                is_pad = torch.cat([torch.zeros(real_len), torch.ones(pad_len)], dim=0)

        # 3. å½’ä¸€åŒ–
        qpos = (qpos - self.stats['qpos_mean']) / self.stats['qpos_std']
        action = (action - self.stats['action_mean']) / self.stats['action_std']

        # 4. å›¾åƒå˜æ¢
        img_tensor = self.transform(img_top)

        return img_tensor, qpos, action, is_pad

# ================= 3. ACT æ¨¡å‹ =================
class ACTModel(nn.Module):
    def __init__(self, state_dim=9, action_dim=8, hidden_dim=256):
        super().__init__()
        # 1. è§†è§‰ Encoder
        resnet = resnet18(weights='DEFAULT')
        self.backbone = nn.Sequential(*list(resnet.children())[:-1])
        self.img_proj = nn.Linear(512, hidden_dim)

        # 2. çŠ¶æ€ Encoder
        self.state_proj = nn.Linear(state_dim, hidden_dim)

        # 3. CVAE Encoder
        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.action_encoder = nn.Linear(action_dim, hidden_dim) 
        self.latent_proj = nn.Linear(hidden_dim, 2 * hidden_dim) 

        # 4. ç­–ç•¥ Decoder
        self.transformer = nn.Transformer(
            d_model=hidden_dim, nhead=4, num_encoder_layers=2, 
            num_decoder_layers=2, dim_feedforward=512, batch_first=True
        )
        
        self.pos_embed = nn.Parameter(torch.randn(CHUNK_SIZE, hidden_dim))
        self.action_head = nn.Linear(hidden_dim, action_dim)
        self.latent_out_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, img, qpos, actions=None, is_pad=None):
        B = img.shape[0]

        # ç‰¹å¾æå–
        img_embed = self.backbone(img).flatten(1) # (B, 512)
        img_embed = self.img_proj(img_embed).unsqueeze(1) # (B, 1, 256)
        state_embed = self.state_proj(qpos).unsqueeze(1)  # (B, 1, 256)

        # CVAE
        mu, logvar = None, None
        if actions is not None:
            # è®­ç»ƒæ¨¡å¼
            action_embed = self.action_encoder(actions) 
            action_summary = torch.mean(action_embed, dim=1, keepdim=True)
            encoder_input = torch.cat([self.cls_token.repeat(B, 1, 1), state_embed, action_summary], dim=1)
            combined_feat = torch.mean(encoder_input, dim=1) 
            
            latent_dist = self.latent_proj(combined_feat)
            mu = latent_dist[:, :256]
            logvar = latent_dist[:, 256:]
            
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + eps * std
        else:
            # æ¨ç†æ¨¡å¼
            z = torch.zeros(B, 256).to(img.device)

        # Decoder
        z_embed = self.latent_out_proj(z).unsqueeze(1)
        src = torch.cat([z_embed, img_embed, state_embed], dim=1)
        query_embed = self.pos_embed.unsqueeze(0).repeat(B, 1, 1)
        
        # ä¿®æ­£ï¼šæ˜ç¡®ä¼ å…¥ src å’Œ tgt
        output = self.transformer(src=src, tgt=query_embed)
        pred_actions = self.action_head(output)
        
        return pred_actions, mu, logvar

# ================= 4. æŸå¤±å‡½æ•° =================
def compute_loss(pred_actions, true_actions, is_pad, mu, logvar):
    # L1 Loss
    all_l1 = nn.functional.l1_loss(pred_actions, true_actions, reduction='none')
    l1 = (all_l1 * (1 - is_pad.unsqueeze(-1))).mean()

    # KL Divergence
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    kl_loss = kl_loss / pred_actions.shape[0]

    total_loss = l1 + KL_WEIGHT * kl_loss
    return total_loss, l1, kl_loss

# ================= 5. è®­ç»ƒå¾ªç¯ =================
def train():
    # 1. å‡†å¤‡ç»Ÿè®¡é‡
    stats = get_dataset_stats(DATA_DIR)

    # 2. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    dataset = ACTDataset(DATA_DIR, stats)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True) 
    
    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = ACTModel().to(DEVICE)
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šåŠ è½½ä¹‹å‰çš„æƒé‡å¹¶å¤„ç†å°ºå¯¸ä¸åŒ¹é… ---
    if RESUME_CHECKPOINT is not None and os.path.exists(RESUME_CHECKPOINT):
        print(f"\nğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡: {RESUME_CHECKPOINT}")
        try:
            # åŠ è½½æƒé‡å­—å…¸
            state_dict = torch.load(RESUME_CHECKPOINT, map_location=DEVICE)
            
            # -----------------------------------------------------------
            # ã€å…³é”®ä¿®æ”¹ã€‘æ™ºèƒ½æƒé‡è£å‰ªé€»è¾‘
            # -----------------------------------------------------------
            if 'pos_embed' in state_dict:
                loaded_len = state_dict['pos_embed'].shape[0]
                current_len = model.pos_embed.shape[0]
                
                if loaded_len != current_len:
                    print(f"âš ï¸ æ£€æµ‹åˆ°é¢„æµ‹æ­¥é•¿ (Chunk Size) å˜åŒ–: æ—§æ¨¡å‹ {loaded_len} -> æ–°æ¨¡å‹ {current_len}")
                    
                    if loaded_len > current_len:
                        print(f"âœ‚ï¸ æ­£åœ¨è‡ªåŠ¨è£å‰ª pos_embed æƒé‡ï¼Œä¿ç•™å‰ {current_len} æ­¥...")
                        # æ ¸å¿ƒåŠ¨ä½œï¼šåˆ‡ç‰‡å–å‰ N ä¸ª
                        state_dict['pos_embed'] = state_dict['pos_embed'][:current_len]
                    else:
                        print(f"âŒ é”™è¯¯: æ–°çš„ CHUNK_SIZE ({current_len}) æ¯”æ—§æ¨¡å‹ ({loaded_len}) å¤§ï¼Œæ— æ³•è¿›è¡Œæƒé‡è£å‰ªï¼")
                        print("å»ºè®®ï¼šå°† CHUNK_SIZE æ”¹å›åŸæ¥çš„å¤§å°ï¼Œæˆ–è€…ä»å¤´è®­ç»ƒã€‚")
                        return

            # åŠ è½½å¤„ç†åçš„æƒé‡
            model.load_state_dict(state_dict) # é»˜è®¤ strict=True
            print("âœ… æƒé‡åŠ è½½æˆåŠŸï¼ˆå·²é€‚é…æ–°æ­¥é•¿ï¼‰ï¼å°†åœ¨è¯¥æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒã€‚")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æƒé‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return
    else:
        if RESUME_CHECKPOINT is not None:
             print(f"\nâš ï¸ è­¦å‘Š: æŒ‡å®šçš„è·¯å¾„ {RESUME_CHECKPOINT} ä¸å­˜åœ¨ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        else:
             print("\nğŸ†• æœªæŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ! æ€»æ ·æœ¬æ•°: {len(dataset)}, Batch Size: {BATCH_SIZE}")
    print(f"ğŸ“… èµ·å§‹ Epoch: {START_EPOCH + 1}, è®¡åˆ’è®­ç»ƒ Epochs: {NUM_EPOCHS}")
    
    model.train()
    
    # --- Epoch å¾ªç¯ ---
    for epoch in range(START_EPOCH, START_EPOCH + NUM_EPOCHS):
        total_l1 = 0
        total_kl = 0
        
        loop = tqdm(dataloader, leave=False)
        for img, qpos, action, is_pad in loop:
            img = img.to(DEVICE)
            qpos = qpos.to(DEVICE)
            action = action.to(DEVICE)
            is_pad = is_pad.to(DEVICE)
            
            optimizer.zero_grad()
            
            pred_actions, mu, logvar = model(img, qpos, action, is_pad)
            
            loss, l1, kl = compute_loss(pred_actions, action, is_pad, mu, logvar)
            
            loss.backward()
            optimizer.step()
            
            total_l1 += l1.item()
            total_kl += kl.item()
            
            loop.set_description(f"Epoch {epoch+1}/{START_EPOCH + NUM_EPOCHS}")
            loop.set_postfix(l1=l1.item(), kl=kl.item())
        
        scheduler.step()
        
        avg_l1 = total_l1 / len(dataloader)
        avg_kl = total_kl / len(dataloader)
        
        print(f"Epoch {epoch+1} | L1 Loss: {avg_l1:.5f} | KL Loss: {avg_kl:.5f}")
            
        # æ¯ 10 è½®ä¿å­˜ä¸€æ¬¡æƒé‡
        if (epoch+1) % 10 == 0:  # è¿™é‡Œæˆ‘æ”¹æˆæ¯10è½®ä¿å­˜ä¸€æ¬¡ï¼Œæ–¹ä¾¿å¾®è°ƒ
            ckpt_path = os.path.join(CKPT_DIR, f"policy_epoch_{epoch+1}.pth")
            torch.save(model.state_dict(), ckpt_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {ckpt_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), os.path.join(CKPT_DIR, "policy_last.pth"))
    print("\nâœ… è®­ç»ƒå…¨éƒ¨å®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    train()