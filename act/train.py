import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import h5py
import os
import pickle
import torchvision.transforms as transforms
from torchvision.models import resnet18
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# ================= é…ç½® =================
DATA_DIR = "data_act"        # æ•°æ®ç›®å½•
CKPT_DIR = "model"           # æ¨¡å‹ä¿å­˜ç›®å½•
BATCH_SIZE = 16              # å»ºè®®è®¾ä¸º 16 æˆ– 32ï¼Œå–å†³äºæ˜¾å­˜å¤§å°
NUM_EPOCHS = 50              # è®­ç»ƒè½®æ•° (å› ä¸ºæ•°æ®é‡å˜å¤§äº†ï¼Œè½®æ•°å¯ä»¥é€‚å½“å‡å°‘ï¼Œæˆ–è€…ä¿æŒ 100)
LR = 1e-4                    # å­¦ä¹ ç‡
CHUNK_SIZE = 100             # é¢„æµ‹æœªæ¥ 100 æ­¥
KL_WEIGHT = 10               # CVAE KL æ•£åº¦æƒé‡

# ================= 0. è®¾å¤‡æ£€æµ‹ä¸æ‰“å° =================
def get_device():
    if torch.cuda.is_available():
        d = torch.device("cuda")
        name = torch.cuda.get_device_name(0)
        print(f"\nâœ… æ£€æµ‹åˆ° GPU: {name}")
        print(f"ğŸš€ å°†ä½¿ç”¨ CUDA è¿›è¡ŒåŠ é€Ÿè®­ç»ƒ\n")
    else:
        d = torch.device("cpu")
        print(f"\nâš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œæ­£åœ¨ä½¿ç”¨ CPU")
        print(f"ğŸ¢ è®­ç»ƒé€Ÿåº¦å¯èƒ½ä¼šè¾ƒæ…¢\n")
    return d

DEVICE = get_device()

if not os.path.exists(CKPT_DIR):
    os.makedirs(CKPT_DIR)

# ================= 1. æ•°æ®é¢„å¤„ç†å·¥å…· =================
def get_dataset_stats(data_dir):
    """ç»Ÿè®¡æ•´ä¸ªæ•°æ®é›†çš„ Mean å’Œ Std"""
    stats_path = os.path.join(CKPT_DIR, "dataset_stats.pkl")
    if os.path.exists(stats_path):
        print(f"ğŸ”„ å‘ç°å·²å­˜åœ¨çš„ç»Ÿè®¡é‡æ–‡ä»¶: {stats_path}ï¼Œç›´æ¥åŠ è½½...")
        with open(stats_path, 'rb') as f:
            return pickle.load(f)

    print("ğŸ“Š æ­£åœ¨è®¡ç®—æ•°æ®é›†ç»Ÿè®¡é‡ (Normalization Stats)...")
    all_qpos = []
    all_action = []
    
    files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.hdf5')]
    
    for f_path in tqdm(files):
        with h5py.File(f_path, 'r') as f:
            all_qpos.append(f['/observations/qpos'][()])
            all_action.append(f['/action'][()])
    
    # æ‹¼æ¥æ‰€æœ‰æ•°æ®
    all_qpos = np.concatenate(all_qpos, axis=0)
    all_action = np.concatenate(all_action, axis=0)
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    stats = {
        'qpos_mean': torch.from_numpy(np.mean(all_qpos, axis=0)).float(),
        'qpos_std': torch.from_numpy(np.std(all_qpos, axis=0)).float(),
        'action_mean': torch.from_numpy(np.mean(all_action, axis=0)).float(),
        'action_std': torch.from_numpy(np.std(all_action, axis=0)).float()
    }
    
    # é˜²æ­¢ std ä¸º 0
    stats['qpos_std'] = torch.clip(stats['qpos_std'], 1e-2, None)
    stats['action_std'] = torch.clip(stats['action_std'], 1e-2, None)
    
    # ä¿å­˜
    with open(stats_path, 'wb') as f:
        pickle.dump(stats, f)
    
    print("âœ… ç»Ÿè®¡å®Œæˆå¹¶ä¿å­˜ï¼")
    return stats

# ================= 2. æ•°æ®é›†ç±» (æ ¸å¿ƒä¿®å¤ç‰ˆ) =================
class ACTDataset(Dataset):
    def __init__(self, data_dir, stats):
        self.stats = stats
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # --- æ„å»ºç´¢å¼• ---
        self.indices = []
        files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.hdf5')])
        
        print("ğŸ“‚ æ­£åœ¨æ‰«ææ•°æ®é›†ï¼Œæ„å»ºç´¢å¼•...")
        for f_path in tqdm(files):
            with h5py.File(f_path, 'r') as f:
                # è·å–è¯¥ Episode çš„æ€»å¸§æ•°
                total_frames = f['/action'].shape[0]
                # å°†æ¯ä¸€å¸§éƒ½åŠ å…¥ç´¢å¼•
                for i in range(total_frames):
                    self.indices.append((f_path, i))
        
        print(f"ğŸ‰ æ•°æ®é›†æ„å»ºå®Œæˆ! å…±æœ‰ {len(files)} ä¸ªæ–‡ä»¶ï¼Œå±•å¼€ä¸º {len(self.indices)} ä¸ªè®­ç»ƒæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        file_path, start_ts = self.indices[idx]
        
        with h5py.File(file_path, 'r') as f:
            # 1. è¯»å–å½“å‰å¸§è§‚æµ‹
            qpos = torch.from_numpy(f['/observations/qpos'][start_ts]).float()
            img_top = f['/observations/images/top'][start_ts] 
            
            # 2. è¯»å–æœªæ¥åŠ¨ä½œåºåˆ—
            action_len = f['/action'].shape[0]
            end_ts = start_ts + CHUNK_SIZE
            
            if end_ts <= action_len:
                # æ•°æ®è¶³å¤Ÿé•¿ï¼Œç›´æ¥åˆ‡ç‰‡
                action = torch.from_numpy(f['/action'][start_ts:end_ts]).float()
                is_pad = torch.zeros(CHUNK_SIZE)
            else:
                # æ•°æ®ä¸å¤Ÿé•¿ï¼Œè¿›è¡Œ Padding
                real_len = action_len - start_ts
                action_real = torch.from_numpy(f['/action'][start_ts:]).float()
                # é‡å¤æœ€åä¸€æ­¥
                last_action = action_real[-1].unsqueeze(0)
                pad_len = CHUNK_SIZE - real_len
                action_pad = last_action.repeat(pad_len, 1)
                
                action = torch.cat([action_real, action_pad], dim=0)
                is_pad = torch.cat([torch.zeros(real_len), torch.ones(pad_len)], dim=0)

        # 3. å½’ä¸€åŒ–
        qpos = (qpos - self.stats['qpos_mean']) / self.stats['qpos_std']
        action = (action - self.stats['action_mean']) / self.stats['action_std']

        # 4. å›¾åƒå˜æ¢
        img_tensor = self.transform(img_top)

        return img_tensor, qpos, action, is_pad

# ================= 3. ACT æ¨¡å‹ =================
class ACTModel(nn.Module):
    def __init__(self, state_dim=9, action_dim=8, hidden_dim=256):
        super().__init__()
        # 1. è§†è§‰ Encoder (ä½¿ç”¨é¢„è®­ç»ƒæƒé‡å¯ä»¥åŠ é€Ÿæ”¶æ•›)
        resnet = resnet18(weights='DEFAULT')
        self.backbone = nn.Sequential(*list(resnet.children())[:-1])
        self.img_proj = nn.Linear(512, hidden_dim)

        # 2. çŠ¶æ€ Encoder
        self.state_proj = nn.Linear(state_dim, hidden_dim)

        # 3. CVAE Encoder
        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.action_encoder = nn.Linear(action_dim, hidden_dim) 
        self.latent_proj = nn.Linear(hidden_dim, 2 * hidden_dim) 

        # 4. ç­–ç•¥ Decoder
        self.transformer = nn.Transformer(
            d_model=hidden_dim, nhead=4, num_encoder_layers=2, 
            num_decoder_layers=2, dim_feedforward=512, batch_first=True
        )
        
        self.pos_embed = nn.Parameter(torch.randn(CHUNK_SIZE, hidden_dim))
        self.action_head = nn.Linear(hidden_dim, action_dim)
        self.latent_out_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, img, qpos, actions=None, is_pad=None):
        B = img.shape[0]

        # ç‰¹å¾æå–
        img_embed = self.backbone(img).flatten(1) # (B, 512)
        img_embed = self.img_proj(img_embed).unsqueeze(1) # (B, 1, 256)
        state_embed = self.state_proj(qpos).unsqueeze(1)  # (B, 1, 256)

        # CVAE
        mu, logvar = None, None
        if actions is not None:
            # è®­ç»ƒæ¨¡å¼
            action_embed = self.action_encoder(actions) 
            action_summary = torch.mean(action_embed, dim=1, keepdim=True)
            encoder_input = torch.cat([self.cls_token.repeat(B, 1, 1), state_embed, action_summary], dim=1)
            combined_feat = torch.mean(encoder_input, dim=1) 
            
            latent_dist = self.latent_proj(combined_feat)
            mu = latent_dist[:, :256]
            logvar = latent_dist[:, 256:]
            
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + eps * std
        else:
            # æ¨ç†æ¨¡å¼
            z = torch.zeros(B, 256).to(img.device)

        # Decoder
        z_embed = self.latent_out_proj(z).unsqueeze(1)
        src = torch.cat([z_embed, img_embed, state_embed], dim=1)
        query_embed = self.pos_embed.unsqueeze(0).repeat(B, 1, 1)
        
        output = self.transformer(src, query_embed)
        pred_actions = self.action_head(output)
        
        return pred_actions, mu, logvar

# ================= 4. æŸå¤±å‡½æ•° =================
def compute_loss(pred_actions, true_actions, is_pad, mu, logvar):
    # L1 Loss (åªè®¡ç®—é Padding éƒ¨åˆ†)
    all_l1 = nn.functional.l1_loss(pred_actions, true_actions, reduction='none')
    l1 = (all_l1 * (1 - is_pad.unsqueeze(-1))).mean()

    # KL Divergence
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    kl_loss = kl_loss / pred_actions.shape[0]

    total_loss = l1 + KL_WEIGHT * kl_loss
    return total_loss, l1, kl_loss

# ================= 5. è®­ç»ƒå¾ªç¯ =================
def train():
    # 1. å‡†å¤‡ç»Ÿè®¡é‡
    stats = get_dataset_stats(DATA_DIR)

    # 2. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    # Windowsä¸‹ num_workers è®¾ç½®ä¸º 0 æ¯”è¾ƒç¨³å¦¥ï¼ŒLinux å¯ä»¥è®¾ç½® 4 æˆ– 8
    dataset = ACTDataset(DATA_DIR, stats)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) 
    
    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = ACTModel().to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ! æ€»æ ·æœ¬æ•°: {len(dataset)}, Batch Size: {BATCH_SIZE}")
    model.train()
    
    for epoch in range(NUM_EPOCHS):
        total_l1 = 0
        total_kl = 0
        
        loop = tqdm(dataloader, leave=False)
        for img, qpos, action, is_pad in loop:
            img = img.to(DEVICE)
            qpos = qpos.to(DEVICE)
            action = action.to(DEVICE)
            is_pad = is_pad.to(DEVICE)
            
            optimizer.zero_grad()
            
            pred_actions, mu, logvar = model(img, qpos, action, is_pad)
            
            loss, l1, kl = compute_loss(pred_actions, action, is_pad, mu, logvar)
            
            loss.backward()
            optimizer.step()
            
            total_l1 += l1.item()
            total_kl += kl.item()
            
            loop.set_description(f"Epoch {epoch+1}/{NUM_EPOCHS}")
            loop.set_postfix(l1=l1.item(), kl=kl.item())
        
        scheduler.step()
        
        avg_l1 = total_l1 / len(dataloader)
        avg_kl = total_kl / len(dataloader)
        
        # æ¯è½®éƒ½æ‰“å°ä¸€æ¬¡ Log
        print(f"Epoch {epoch+1} | L1 Loss: {avg_l1:.5f} | KL Loss: {avg_kl:.5f}")
            
        # æ¯ 10 è½®ä¿å­˜ä¸€æ¬¡æƒé‡
        if (epoch+1) % 10 == 0:
            ckpt_path = os.path.join(CKPT_DIR, f"policy_epoch_{epoch+1}.pth")
            torch.save(model.state_dict(), ckpt_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {ckpt_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), os.path.join(CKPT_DIR, "policy_last.pth"))
    print("\nâœ… è®­ç»ƒå…¨éƒ¨å®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    train()